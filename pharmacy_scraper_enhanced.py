#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åšç”ŸåŠ´åƒçœ åŒ»ç™‚æƒ…å ±ãƒãƒƒãƒˆ è–¬å±€ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ (æ”¹å–„ç‰ˆ)

æ–°æ©Ÿèƒ½:
- ãƒ­ã‚®ãƒ³ã‚°æ©Ÿèƒ½ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ï¼‰
- é€²æ—ç‡è¡¨ç¤º
- å®Ÿè¡Œæ™‚é–“è¨ˆæ¸¬
- çµ±è¨ˆæƒ…å ±ã®å‡ºåŠ›
- ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã®çµ±ä¸€
- ã‚¨ãƒ©ãƒ¼è©³ç´°æƒ…å ±
"""
import time
import csv
import os
import json
import re
import random
import logging
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
from webdriver_manager.chrome import ChromeDriverManager

# ============================================================
# è¨­å®š
# ============================================================
BASE_URL = "https://www.iryou.teikyouseido.mhlw.go.jp/znk-web/juminkanja/S2300/initialize"
OUTPUT_DIR = "pharmacy_data_enhanced"
PROGRESS_FILE = os.path.join(OUTPUT_DIR, "progress.json")
LOG_FILE = os.path.join(OUTPUT_DIR, "scraper.log")
STATS_FILE = os.path.join(OUTPUT_DIR, "statistics.json")

# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šï¼ˆçµ±ä¸€ï¼‰
WAIT_TIMEOUT = 15
ELEMENT_TIMEOUT = 10
PAGE_LOAD_TIMEOUT = 20

# å¾…æ©Ÿæ™‚é–“
MIN_WAIT = 2.0
MAX_WAIT = 4.0

# ãƒªãƒˆãƒ©ã‚¤è¨­å®š
MAX_RETRIES = 3

PREFECTURES = {
    "01": "åŒ—æµ·é“", "02": "é’æ£®çœŒ", "03": "å²©æ‰‹çœŒ", "04": "å®®åŸçœŒ", "05": "ç§‹ç”°çœŒ",
    "06": "å±±å½¢çœŒ", "07": "ç¦å³¶çœŒ", "08": "èŒ¨åŸçœŒ", "09": "æ ƒæœ¨çœŒ", "10": "ç¾¤é¦¬çœŒ",
    "11": "åŸ¼ç‰çœŒ", "12": "åƒè‘‰çœŒ", "13": "æ±äº¬éƒ½", "14": "ç¥å¥ˆå·çœŒ", "15": "æ–°æ½ŸçœŒ",
    "16": "å¯Œå±±çœŒ", "17": "çŸ³å·çœŒ", "18": "ç¦äº•çœŒ", "19": "å±±æ¢¨çœŒ", "20": "é•·é‡çœŒ",
    "21": "å²é˜œçœŒ", "22": "é™å²¡çœŒ", "23": "æ„›çŸ¥çœŒ", "24": "ä¸‰é‡çœŒ", "25": "æ»‹è³€çœŒ",
    "26": "äº¬éƒ½åºœ", "27": "å¤§é˜ªåºœ", "28": "å…µåº«çœŒ", "29": "å¥ˆè‰¯çœŒ", "30": "å’Œæ­Œå±±çœŒ",
    "31": "é³¥å–çœŒ", "32": "å³¶æ ¹çœŒ", "33": "å²¡å±±çœŒ", "34": "åºƒå³¶çœŒ", "35": "å±±å£çœŒ",
    "36": "å¾³å³¶çœŒ", "37": "é¦™å·çœŒ", "38": "æ„›åª›çœŒ", "39": "é«˜çŸ¥çœŒ", "40": "ç¦å²¡çœŒ",
    "41": "ä½è³€çœŒ", "42": "é•·å´çœŒ", "43": "ç†Šæœ¬çœŒ", "44": "å¤§åˆ†çœŒ", "45": "å®®å´çœŒ",
    "46": "é¹¿å…å³¶çœŒ", "47": "æ²–ç¸„çœŒ"
}

# ============================================================
# ãƒ­ã‚°è¨­å®š
# ============================================================
def setup_logging():
    """ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # ãƒ­ã‚¬ãƒ¼ã®ä½œæˆ
    logger = logging.getLogger('PharmacyScraper')
    logger.setLevel(logging.DEBUG)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©
    fh = logging.FileHandler(LOG_FILE, encoding='utf-8')
    fh.setLevel(logging.DEBUG)
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    
    # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    
    logger.addHandler(fh)
    logger.addHandler(ch)
    
    return logger

logger = setup_logging()

# ============================================================
# çµ±è¨ˆæƒ…å ±ç®¡ç†
# ============================================================
class Statistics:
    """çµ±è¨ˆæƒ…å ±ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.start_time = datetime.now()
        self.total_pharmacies = 0
        self.total_with_data = 0
        self.total_without_data = 0
        self.errors = 0
        self.skipped = 0
        self.prefecture_stats = {}
    
    def add_pharmacy(self, pref_code, has_data=True):
        """è–¬å±€ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²"""
        self.total_pharmacies += 1
        if has_data:
            self.total_with_data += 1
        else:
            self.total_without_data += 1
        
        if pref_code not in self.prefecture_stats:
            self.prefecture_stats[pref_code] = {'total': 0, 'with_data': 0}
        
        self.prefecture_stats[pref_code]['total'] += 1
        if has_data:
            self.prefecture_stats[pref_code]['with_data'] += 1
    
    def add_error(self):
        """ã‚¨ãƒ©ãƒ¼æ•°ã‚’è¨˜éŒ²"""
        self.errors += 1
    
    def add_skip(self):
        """ã‚¹ã‚­ãƒƒãƒ—æ•°ã‚’è¨˜éŒ²"""
        self.skipped += 1
    
    def save(self):
        """çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜"""
        elapsed = (datetime.now() - self.start_time).total_seconds()
        
        stats_data = {
            'execution_time_seconds': elapsed,
            'execution_time_human': str(datetime.now() - self.start_time),
            'total_pharmacies': self.total_pharmacies,
            'total_with_prescription_data': self.total_with_data,
            'total_without_prescription_data': self.total_without_data,
            'errors': self.errors,
            'skipped_duplicates': self.skipped,
            'prefecture_stats': self.prefecture_stats,
            'completed_at': datetime.now().isoformat()
        }
        
        with open(STATS_FILE, 'w', encoding='utf-8') as f:
            json.dump(stats_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {STATS_FILE}")
    
    def print_summary(self):
        """çµ±è¨ˆã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        elapsed = (datetime.now() - self.start_time).total_seconds()
        
        print("\n" + "="*60)
        print("ğŸ“Š å®Ÿè¡Œçµ±è¨ˆ")
        print("="*60)
        print(f"å®Ÿè¡Œæ™‚é–“: {elapsed/3600:.2f}æ™‚é–“")
        print(f"ç·è–¬å±€æ•°: {self.total_pharmacies:,}ä»¶")
        print(f"  â”” å‡¦æ–¹ç®‹æ•°ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š: {self.total_with_data:,}ä»¶")
        print(f"  â”” å‡¦æ–¹ç®‹æ•°ãƒ‡ãƒ¼ã‚¿ãªã—: {self.total_without_data:,}ä»¶")
        print(f"ã‚¨ãƒ©ãƒ¼æ•°: {self.errors}ä»¶")
        print(f"ã‚¹ã‚­ãƒƒãƒ—æ•°: {self.skipped}ä»¶")
        print("="*60)

stats = Statistics()

# ============================================================
# ãƒ‰ãƒ©ã‚¤ãƒãƒ¼è¨­å®š
# ============================================================
def setup_driver():
    """Webãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®è¨­å®š"""
    options = webdriver.ChromeOptions()
    options.add_argument('--headless=new')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36')
    options.add_argument("--log-level=3")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)
    
    logger.info("Webãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    return driver

# ============================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ============================================================
def get_existing_ids(filepath):
    """æ—¢å­˜ã®CSVã‹ã‚‰è–¬å±€IDã‚’å–å¾—"""
    if not os.path.exists(filepath):
        return set()
    ids = set()
    try:
        with open(filepath, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            row_count = 0
            for row in reader:
                row_count += 1
                if row.get('id'):
                    ids.add(row['id'])
            if row_count == 0:
                return set()
    except Exception as e:
        logger.error(f"æ—¢å­˜IDã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {filepath} - {e}")
    return ids

def is_csv_valid(filepath):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæœ‰åŠ¹ã‹ç¢ºèª"""
    if not os.path.exists(filepath):
        return False
    try:
        with open(filepath, 'r', encoding='utf-8-sig') as f:
            return len(f.readlines()) > 1
    except:
        return False

def random_sleep():
    """ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿ"""
    time.sleep(random.uniform(MIN_WAIT, MAX_WAIT))

def append_to_csv(filename, data_dict):
    """CSVã«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½è¨˜"""
    file_exists = os.path.exists(filename)
    with open(filename, 'a', newline='', encoding='utf-8-sig') as f:
        fieldnames = ['id', 'name', 'address', 'prescription_count', 'prefecture', 'scraped_at']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()
        writer.writerow(data_dict)

# ============================================================
# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°
# ============================================================
def safe_set_value(driver, element_id, value):
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã®å€¤ã‚»ãƒƒãƒˆ"""
    for i in range(MAX_RETRIES):
        try:
            element = WebDriverWait(driver, ELEMENT_TIMEOUT).until(
                EC.presence_of_element_located((By.ID, element_id))
            )
            driver.execute_script("""
                arguments[0].value = arguments[1];
                arguments[0].dispatchEvent(new Event('change', { bubbles: true }));
                arguments[0].dispatchEvent(new Event('input', { bubbles: true }));
            """, element, value)
            return True
        except (StaleElementReferenceException, TimeoutException):
            time.sleep(1)
            continue
        except Exception as e:
            logger.warning(f"inputè¨­å®šå†è©¦è¡Œä¸­({i+1}/{MAX_RETRIES}): {e}")
            time.sleep(1)
    return False

def extract_prescription_count(driver, detail_url, pharmacy_id, pharmacy_name):
    """å‡¦æ–¹ç®‹æ•°ã‚’æŠ½å‡º"""
    try:
        driver.get(detail_url)
        WebDriverWait(driver, WAIT_TIMEOUT).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        try:
            target = driver.find_element(
                By.XPATH, 
                "//th[contains(text(), 'ç·å–æ‰±å‡¦æ–¹ç®‹æ•°')]/following-sibling::td"
            )
            text = target.text.strip()
            match = re.search(r'(\d+(?:,\d+)?)', text)
            if match:
                return match.group(1)
        except NoSuchElementException:
            logger.debug(f"å‡¦æ–¹ç®‹æ•°ãªã—: {pharmacy_id} - {pharmacy_name}")
    except Exception as e:
        logger.error(f"è©³ç´°ãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {pharmacy_id} - {pharmacy_name} - {e}")
        stats.add_error()
    return ""

def setup_search_conditions(driver, pref_code, pref_name):
    """æ¤œç´¢æ¡ä»¶ã®è¨­å®š"""
    for attempt in range(MAX_RETRIES):
        try:
            driver.get(BASE_URL)
            WebDriverWait(driver, WAIT_TIMEOUT).until(
                EC.presence_of_element_located((By.ID, "todofukenCd"))
            )
            
            if not safe_set_value(driver, "todofukenCd", pref_code):
                raise Exception("éƒ½é“åºœçœŒã‚»ãƒƒãƒˆå¤±æ•—")
            time.sleep(1)
            
            if not safe_set_value(driver, "iryoKikanShubetsuCd", "5"):
                raise Exception("åŒ»ç™‚æ©Ÿé–¢ç¨®åˆ¥ã‚»ãƒƒãƒˆå¤±æ•—")
            time.sleep(1)
            
            btn = WebDriverWait(driver, ELEMENT_TIMEOUT).until(
                EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'æ¤œç´¢')]"))
            )
            driver.execute_script("arguments[0].click();", btn)
            
            WebDriverWait(driver, WAIT_TIMEOUT).until(
                EC.presence_of_element_located((By.CLASS_NAME, "result-count"))
            )
            
            logger.info(f"{pref_name}: æ¤œç´¢æ¡ä»¶è¨­å®šæˆåŠŸ")
            return True
            
        except Exception as e:
            logger.warning(f"{pref_name}: æ¤œç´¢æ¡ä»¶è¨­å®šã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt+1}/{MAX_RETRIES}) - {e}")
            time.sleep(3)
    
    logger.error(f"{pref_name}: æ¤œç´¢æ¡ä»¶è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ")
    return False

def scrape_prefecture(driver, pref_code, pref_name, progress_data):
    """éƒ½é“åºœçœŒã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    logger.info(f"{'='*60}")
    logger.info(f"ğŸ¥ {pref_name} ({pref_code}) é–‹å§‹")
    logger.info(f"{'='*60}")
    
    csv_filename = os.path.join(OUTPUT_DIR, f"{pref_code}_{pref_name}_prescription.csv")
    
    # å®Œäº†ãƒã‚§ãƒƒã‚¯
    if not is_csv_valid(csv_filename) and progress_data.get(pref_code) == "DONE":
        logger.info(f"{pref_name}: å®Œäº†è¨˜éŒ²ãŒã‚ã‚Šã¾ã™ãŒã€ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚å†å–å¾—ã—ã¾ã™")
    elif progress_data.get(pref_code) == "DONE":
        logger.info(f"{pref_name}: ãƒ‡ãƒ¼ã‚¿å–å¾—æ¸ˆã¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        return
    
    existing_ids = get_existing_ids(csv_filename)
    if existing_ids:
        logger.info(f"{pref_name}: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ {len(existing_ids)}ä»¶ (ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™)")
    
    # æ¤œç´¢ç”»é¢ã®è¨­å®š
    if not setup_search_conditions(driver, pref_code, pref_name):
        logger.error(f"{pref_name}: æ¤œç´¢æ¡ä»¶ã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
    
    page_num = 1
    prefecture_count = 0
    
    while True:
        logger.info(f"{pref_name}: ãƒšãƒ¼ã‚¸ {page_num} å‡¦ç†ä¸­...")
        
        pharmacy_list = []
        try:
            rows = driver.find_elements(By.XPATH, "//table[contains(@class, 'result-table')]/tbody/tr")
            for row in rows:
                cols = row.find_elements(By.TAG_NAME, "td")
                if len(cols) >= 3:
                    p_id = cols[0].text.strip()
                    p_name = cols[1].text.strip()
                    p_addr = cols[2].text.strip()
                    try:
                        link = cols[1].find_element(By.TAG_NAME, "a").get_attribute("href")
                        pharmacy_list.append({
                            "id": p_id, "name": p_name, "address": p_addr, "url": link
                        })
                    except:
                        pass
        except Exception as e:
            logger.error(f"{pref_name}: ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼ - {e}")
            break
        
        if not pharmacy_list:
            logger.warning(f"{pref_name}: ãƒ‡ãƒ¼ã‚¿ãªã—ã€ã¾ãŸã¯å–å¾—çµ‚äº†")
            break
        
        current_list_url = driver.current_url
        
        for p in pharmacy_list:
            if p['id'] in existing_ids:
                stats.add_skip()
                continue
            
            count = extract_prescription_count(driver, p['url'], p['id'], p['name'])
            
            if count:
                logger.info(f"{pref_name}: {p['name'][:20]}... âœ… {count}ä»¶")
                save_data = {
                    'id': p['id'],
                    'name': p['name'],
                    'address': p['address'],
                    'prescription_count': count,
                    'prefecture': pref_name,
                    'scraped_at': datetime.now().isoformat()
                }
                append_to_csv(csv_filename, save_data)
                existing_ids.add(p['id'])
                stats.add_pharmacy(pref_code, has_data=True)
                prefecture_count += 1
            else:
                logger.debug(f"{pref_name}: {p['name'][:20]}... ãƒ¼")
                stats.add_pharmacy(pref_code, has_data=False)
            
            random_sleep()
        
        # æ¬¡ãƒšãƒ¼ã‚¸ã¸
        driver.get(current_list_url)
        try:
            WebDriverWait(driver, WAIT_TIMEOUT).until(
                EC.presence_of_element_located((By.CLASS_NAME, "result-table"))
            )
            
            next_links = driver.find_elements(By.XPATH, "//a[contains(text(), 'æ¬¡ã¸')]")
            if not next_links:
                break
            
            parent_li = next_links[0].find_element(By.XPATH, "..")
            if "disabled" in parent_li.get_attribute("class"):
                break
            
            driver.execute_script("arguments[0].click();", next_links[0])
            time.sleep(3)
            page_num += 1
            
        except Exception as e:
            logger.info(f"{pref_name}: ãƒšãƒ¼ã‚¸é€ã‚Šçµ‚äº† - {e}")
            break
    
    # å®Œäº†è¨˜éŒ²
    progress_data[pref_code] = "DONE"
    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
        json.dump(progress_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"{pref_name}: å®Œäº† - å–å¾—æ•° {prefecture_count}ä»¶")

# ============================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================================================
def calculate_progress(progress_data):
    """é€²æ—ç‡ã‚’è¨ˆç®—"""
    completed = sum(1 for v in progress_data.values() if v == "DONE")
    total = len(PREFECTURES)
    percentage = (completed / total) * 100
    return completed, total, percentage

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    logger.info("="*60)
    logger.info("ğŸ¥ åšç”ŸåŠ´åƒçœ åŒ»ç™‚æƒ…å ±ãƒãƒƒãƒˆ è–¬å±€ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ (æ”¹å–„ç‰ˆ)")
    logger.info("="*60)
    
    driver = setup_driver()
    
    # é€²æ—èª­ã¿è¾¼ã¿
    progress = {}
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
            progress = json.load(f)
        completed, total, percentage = calculate_progress(progress)
        logger.info(f"ğŸ“Š é€²æ—çŠ¶æ³: {completed}/{total}éƒ½é“åºœçœŒå®Œäº† ({percentage:.1f}%)")
    
    try:
        for code, name in PREFECTURES.items():
            scrape_prefecture(driver, code, name, progress)
            
            # é€²æ—è¡¨ç¤º
            completed, total, percentage = calculate_progress(progress)
            logger.info(f"ğŸ“Š å…¨ä½“é€²æ—: {completed}/{total}éƒ½é“åºœçœŒå®Œäº† ({percentage:.1f}%)")
        
        logger.info("âœ… å…¨éƒ½é“åºœçœŒã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
    except KeyboardInterrupt:
        logger.warning("âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
    finally:
        driver.quit()
        stats.save()
        stats.print_summary()
        logger.info("ãƒ–ãƒ©ã‚¦ã‚¶ã‚’çµ‚äº†ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
